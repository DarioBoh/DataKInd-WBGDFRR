\documentclass[12pt, a4paper, oneside, headinclude, footinclude]{article}

\input{structure.tex}

\title{\normalfont\spacedallcaps{Image analysis for disaster recovery, A DataKind report for the World Bank GFDRR}}

\author{\spacedlowsmallcaps{Krishna Bhogaonker \& Patrick Doupe}} 

\date{} 

\begin{document}

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} 
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} 

\pagestyle{scrheadings} 

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle 

\setcounter{tocdepth}{2}

\tableofcontents 

\listoffigures 

\listoftables

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\section*{Abstract}

We discuss how the World Bank can use machine learning and satellite images to
improve disaster relief efforts. We include a review of image analysis with
convolutional neural networks. These networks are illustrated with code
examples using the Keras deep learning library. 

%----------------------------------------------------------------------------------------
%	AUTHOR AFFILIATIONS
%----------------------------------------------------------------------------------------

%\let\thefootnote\relax\footnotetext{* \textit{}}

%\let\thefootnote\relax\footnotetext{\textsuperscript{1} \textit{}}

%----------------------------------------------------------------------------------------

\newpage 

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction}

The capacity for computers to take images and return useful information has
grown over the last decade. We have trained models detect numbers, to
distinguishing between cats and dogs and to segmenting images by objects. In
this article, we review this literature.

The review's guiding question is \textit{how can we use
images and computers to identify areas at risk in a crisis}. We do this in
two ways. First, by presentiting an intuitive understanding of various deep
learning models and model types; second, by presenting applications of models
using these methods and satellite images to generate insights. The philosophy
of this review is not that the reader should expect to know how to build
working prototypes, rather to understand them in sufficient detail so that
they can better collaborate with trained researchers. We provide Python code and
links to simple tutorials so that the reader can obtain a feel for how these
things work. We present code rather than math. For a textbook treatment into
deep learning, try~\cite{lecun2015deep}.

The code is written in the Python language (version 3.7). This language is
standard in both research and production of deep learning models. For an
introduction to Python for economists, we recommend this tutorial by two
top
scholars~\url{https://lectures.quantecon.org/py/index_learning_python.html}

%----------------------------------------------------------------------------------------
%	METHODS
%----------------------------------------------------------------------------------------

\subsection{Deep learning Frameworks}

In addition to many languages, there are many different deep learning
libraries (or frameworks). We focus in this review on \texttt{Keras}. We
make this choice because of Keras' ease of use and interpretability. 

There are many other libraries and frameworks. It is difficult to get hard numbers
about usage because much of this is in industry. There are rough
audiences for each library. TensorFlow is useful in research and production.
There is a high level of boilerplate required and this is not for beginners.
In fact, \texttt{Keras} (incorporated into \texttt{TensorFlow}) is recommended
(~\url{https://www.tensorflow.org/tutorials/}). \texttt{PyTorch} markets
itself for fast, flexible experimentation.\footnote{One of the authors is a
big fan of PyTorch.} Version 1.0 will provide support for building models in
production. Both \texttt{TensorFlow} and \texttt{PyTorch} are about as fast as
each other, and both are faster thank \texttt{Keras}
(~\url{https://wrosinski.github.io/deep-learning-frameworks/}). The trade off
being ease of use versus speed. Other languages include the former most
popular \texttt{Caffe}, Microsoft's \texttt{CNTK} and Apache's \texttt{MXNet}.

Below is an example linear regression model with ten explanatory variables.
We begin with importing some objects: an Input object which defines the shape
of the explanatory varables; a Dense object which maps the data from the input
shape to the output shape; a Model object which combines the two. Short of
comment lines and importing data, we can run a regression model in under ten
lines. Running simple deep networks is a matter of adding additional
components.

\begin{minted}[
frame=lines,
linenos
]{python}
# import functions
from keras.layers import Input
from keras.layers import Dense
from keras.models import Model

# import data
X = ...
y = ...

# set amount of explanatory (x) variables 
inputs = Input(shape=(10,))
# set outcome (y) variable
predictions = Dense(1, activation='linear')(inputs)
# define the model
model = Model(inputs=inputs, outputs=predictions)
# set loss function and optimizer
model.compile(optimizer='rmsprop', loss='rmse', metrics=['accuracy'])
# starts training
model.fit(X, y)  
\end{minted}

We discuss the arguments of \texttt{Dense} and \texttt{compile} below. 

\section{Convolutional Neural Networks for computer vision tasks}

Why don't we use linear regression?  An image is a three dimensional matrix of
numbers. One dimension is the width (W) of the image, one is the height (H) of
the image, and we typically have red, green and blue spectral bands. That is,
we have $H\times W \times 3$ numbers in an image. We can flatten the three
dimensions to one and feed this into a multinomial logistic regression model.
The model can be implemented by making the following code changes.

\begin{minted}[
frame=lines,
linenos
]{python}
\ldots
H = \ldot               # height of images
W = \ldots              # width
num\_classes = \ldots   # number of classes to classify  
inputs = Input(shape=(W * H * 3,))
# set outcome (y) variable
predictions = Dense(num\_classes, activation='sigmoid')(inputs)
\ldots
model.compile(optimizer='rmsprop', 
              loss='categorical\_crossentropy', 
              metrics=['accuracy'])
\ldots
\end{minted}

This will work one some problems (), but we can do better by exploiting
characteristics of the data. Images are different from other classification
problems, as the value of a pixel is highly correlated with neighbouring
pixels' values. Where neighbouring pixels' values are different, this contains
meaning --- often this indicates an edge.

Convolutional networks exploit the spatial autocorrelation in image data by
taking $k\times k \times n$ \textit{filters} of data to generate features.
Here $k$ is some integer (typically between 1 and 11), and $n$ is the depth of
the input.  For RGB images $n$=3, but for satellite images with more bands, we
can have $n>3$. 
T
A filter starts at the top corner, computes the dot product between the filter
and the associated corner of the image. This generates a single number. The
filter is moved a few pixels to the right and we again take the dot product.
Sweeping across the image we generate a two dimensional matrix. An additional
filter makes this a three dimensional matrix. The depth of this matrix is the
number of filters.\footnote{For a good visual display of this process, see
\url{http://cs231n.github.io/assets/conv-demo/index.html}.}

To stack layers together for deeper models, there are three  additional core
components. First, to allow the model to find non linear relationships, models
will often include a non linear \textit{activation} function after the dot
product and before the next step. Common activation functions include the
\texttt{sigmoid, tanh and ReLU} functions. The \texttt{ReLU} is popular
because it is fast and effective. For a dot product output $x$, the
\texttt{ReLU} is $\max\{0, x\}$. 

Second, to reduce the spatial size of the output, a \textit{pooling} reduces a
$\ell\times\ell$ patch of the output to a single number, using the $\max$
operator. Pooling reduces the amount of parameters in the model and helps
against overfitting the data (Ref). Last, to control the image sizes through
the network, a \textit{pad} of zero entry cells is often used around the input
image. 

The bulk of a convolutional neural network is stacking these layers on top of
each other. After stacking these layers on top of each other, we flatten the
output to a one dimensional layer. Dense layers are added on top. 

\begin{minted}[
frame=lines,
linenos
]{python}
    # load libraries
from keras.models import Model, Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Flatten, Input
from keras.layers import LSTM, Embedding, Dense

model = Sequential()
    # add first convolutional layer with 64 filters of size 3 x 3
    # and padding to retain image size
    # and ReLU activation function
model.add(Conv2D(64, (3, 3), 
          activation='relu', 
          padding='same', 
          input_shape=(224, 224, 3)))
    # add second convolutional layer with 64 filters of size 3 x 3
    # and ReLU activation function
    # no padding
model.add(Conv2D(64, (3, 3), 
          activation='relu'))
    # add pooling
model.add(MaxPooling2D((2, 2))
    # ...
    # a few more layers
    # ...
model.add(Flatten())
model.add(Dense(4096, activation='sigmoid')(inputs)
model.add(Dense(4096, activation='sigmoid')(inputs)
    # ...

\end{minted}

\subsection{Training models}

The question is then: how to train these (hundreds of) millions of parameters?
The high level answer has two steps. First, observe the \textit{error}
between predictions and actual labels. The \textit{loss} is a function of the
error. For instance, one loss is the error squared. The second step is to pass
this loss through the model, updating parameters that cause the most loss.

This second step is known as \texttt{gradient descent}. Gradient descent is a
way of finding the minimum of a function, in our case, the loss function. We
take the gradient of the loss function and update the weights by the negative
of the gradient (see code block below). 

\begin{minted}
[
frame=lines,
linenos
]{python}
# Vanilla Gradient Descent

while True:
  weights\_grad = evaluate\_gradient(loss\_fun, data, weights)
    # perform parameter update
  weights += - step\_size * weights\_grad
\end{minted}

Since we often train on many thousands (millions) of images, we cannot compute
this in memory. Insted, a small batch of images is used and the model updated.
This is called \texttt{batch gradient descent}. There is also stochastic
gradient descent for  batches of size one observation. With enough batches,
we obtain good results.

Of course, we don't have to hard code the \texttt{evaluate\_gradient}
function, we can rely on libraries to do this for us. We saw this in code
block X above. We have the \texttt{sgd}, or stochastic gradient descent
optimiser and the \texttt{rmse} or root mean squared error loss function.

\begin{minted}[
frame=lines,
linenos
]{python}
# set loss function and optimizer
model.compile(optimizer='sgd', 
              loss='rmse')
\end{minted}


Also unsupervised learning. Feature extraction.

Transfer learning.

\subsection{Why convolutional neural networks}

1. Pixels are not independent of one another
2. With pictures, only relative position matters [Spatially stationary
statistics]
3. Much fewer neurons.

Results of image analysis

\section{Methods}



\subsubsection{If you understand VGG-net, you're 80 per cent there}

VGG net is super simple and easy to explain. Started deep learning revolution.

VGG Net~\cite{SimonyanZ14a}
\begin{itemize}
    \item Increased depth (16--19 layers) with smaller filters
    \item First/Second places at ImageNet2014
    \item Widely used as a base: need references.
\end{itemize}

Repetative blocks: see appendix
\begin{itemize}
    \item convolutional: non independence of pixels
    \item zero padding: to maintain shape
    \item pooling: to extract most useful information
    \item flattening: extract all of the information
\end{itemize}

It's plug and play

\begin{minted}{python}
from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np

model = VGG16(weights='imagenet', include_top=True)

img_path = 'remote_area_example.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

features = model.predict(x)
\end{minted}

Load \texttt{VGG16} object. The argument \texttt{include\_top=False} means that we exclude
the top layers. 

\subsection{Pretraining}

Unsupervised learning -- feature extraction. Benefit: others have used
millions of images. These models extract information

Super common strategy with limited information. You extract features and then
only train top 512, 1024, 2048, 4096x2 layers. Very useful (our work with
population, https://arxiv.org/abs/1510.00098).

evidence??

When you have lots of data and access to GPUs and your data is different to
ImageNet, then there are advantages to training your own.

When you have a small amount of labelled data, this works well.

\section{Literature}

\subsection{Image classification}

LeNet (http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
Conv => Pool => Conv => Pool => Flattened => prediction

Krizhevsky ImageNet~\cite{NIPS2012_4824}
\begin{itemize}
    \item Used dropout to prevent overfitting
    \item ReLu
    \item We start to go deep: 5 convolutional layers
    \item Got best results to that date on a hard problem
\end{itemize}

ResNet~\cite{he2016deep}

\begin{itemize}
    \item Residual learning framework
    \item Degradation became an issue with deep models. This makes no sense,
        you could just have a shallower model with identity mappings. So
        residual networks contain the identity mapping.
    \item Crazy large: 152 layers but with lower `complexity'
    \item Then state of the art.
\end{itemize}

Current state of the art: ensembles
(http://image-net.org/challenges/LSVRC/2017/results)

\subsection{Object detection}

Classification + localising objects. The challenge: ``detect a \textit{potentially large
number of object instances with varying sizes in the same image} using a
limited amount of computing resources.''~\cite[Their emphasis]{NIPS2013_5207}

Super basic model: slide a classifier across an image. This will be very slow.

Potentially
useful~\url{https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606}

\textbf{Using regression to do this}~\cite{NIPS2013_5207}

The DNN based regression outputs a binary mask of the bounding box. 

Seven layer network: five convolutional and two fully connected. ReLU and max
pooling. Note that output much smaller than input.

Instead of a softmax, they have a regression layer that outputs a binary mask:
1 if image inside the box, 0 outside.\ 

p. 3 Because base network highly non convex -> no guarantee for optimal solution ->
regularisation.

Regularisation: objects are small relative to image size, so an output of all
0s is super common. Use regularisation to increase weights of non zero
outputs.

Have five networks: one for box predictions, four others for {top, bottom,
left, right}. 
It is possible to share layers. 

Since output smaller than inputs, need to upscale

The details regarding refinement etc are a little confusing. 

Training: model needs to be trained with a huge amount of training data:
objects of different sizes need to occur at almost every location. << is this
an issue for satellite monitoring?

Pre training on classification task used.

Need to train a single model per object type and mask type.

Doesn't work super well: mAP = 30.5\%

\textbf{RCNN}~\cite{Girshick2014, Girshick2015}

Regions with CNN features
Improved on mAP (mean Average Precision) by more than 30\%. They get mAP of
53.3\%. 

Idea
\begin{enumerate}
    \item extract some 2k bounding boxes (region proposals)
    \item warp boxes and run through CNN and SVM to classify
\end{enumerate}

Use `recognition using regions' paradigm.
Alternatives: Regression like above, sliding windows.

A challenge --- labelled data is scarce. Common answer is to do unsupervised
pre training followed by supervised fine tuning. They also use supervised pre
training on a large dataset.

There are many options for proposing regions, RCNN uses `selective-search'
They also use VGG net for features.

RCNN is multi stage, expensive and s l o w. `R-CNN is slow because it performs
a ConvNet forward pass for each object proposal, without sharing computation.
(fast RCNN paper)'

\textbf{Fast-RCNN} single stage training algorithm. 

Spatial Pyramid Pooling Networks (SPPnets). SPPnets compute a feature map
for image, classifies each object proposal using shared features. But this is
a multi stage pipeline.

Multi task loss function
    (a) classification
    (b) 4d regression for bounding box

It's faster than SPPnet/RCNN but there is still a bottle neck of region
proposals

Use a single network for feature extraction, classification and bounding box.


\textbf{Faster RCNN}~\cite{Ren2017}

Because of proposal bottleneck: they propose a `Region Proposal Network.' RPNs
share layers with the object detection networks --- this allows a speed up.
E.g.\ from 2 or 0.2s per image to 10ms per image.

The RPN is basically (?) an attention network.

RPN takes anchors and (1) is this an object (2) can we adjust the anchor box
a little to get better fit (rather than absolute corners, predict change from
anchor box corners).

FasterRCNN is trainable end to end.

So at the end you get a set of overlapping proposals. If we have two
overlapping images, discard one with lower classification score.

\url{https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/}
Predicing (xmin, xmax, ymin, ymax) is hard. For instance, how to enforce
xmin < xmax

Potential code:
\url{https://github.com/jinfagang/keras\_frcnn/blob/master/keras\_frcnn/vgg.py}

Useful overview:
\url{https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4}

\textbf{Mask RCNN}~\cite{he2017}

We can do segmentation as well! Have a third branch that allows segmentation. 

But there is a slight misalignment between bounding boxes and pixels because
of pixel integers. The original image is say 200 $\times$ 200 and the feature map is
say 30 * 30. So to select the top 15*15 corner we need 15 * 30 / 200 $\approx$
2.25 pixels. RoIPool uses 2$\times$2. RoIAlign (this paper's innovation) uses
bilinear interpolation to get an idea of what the 2.25th pixel is.


\textbf{R-FCN}~\cite{NIPS2016_6465}

The above apply a costly per region subnetwork hundreds of times :(

\textit{Translation invariance} - the object can be anywhere in an image for
correct classification

\textit{Translation variance} - if we move the object, the bounding box should
change. 

Use an RPN for proposals.

\textbf{YOLO}~\cite{redmon2016yolo}

\url{http://www.youtube.com/watch?v=NM6lrxy0bxs}

YOLO is super fast. Not most accurate tho. Might be useful for large satellite
areas.

YOLO generalises to new domains (e.g. art)

Divide image into S $\times$ S grid. 
For each grid, 
    1. have B bounding boxes. 
    For each bounding box, 
        predict 5 components (4 spatial {x, y, w, h}, 1 confidence). 
        (x, y) -- center of the box relative to the bounds of the grid cell. 
        (w, h) -- width and height relative to whole image
        confidenc -- IOU between predicted and ground truth.
    2. Predict C conditional class probabilities
       Prob(Class | Object)
       Only one set of class probabilities regardless of the number of boxes
       B.

At test time: the confidence * class probabilities for class specific
confidence scores for each box.

Use Google LeNet.

Limitations: 
1. strong spatial constraints -- hard coded grid size and number of
bounding boxes and classes per box. Many small birds will be hard to predict.
--- also houses ---

2. struggles to generalise to objects with new aspect ratios. so will want a
variety of house angles. may be difficult to work with damaged buildings 

YOLO v2: big step was using imagenet and backproping classification on this
data

\textbf{SSD}~\cite{liu2016ssd}

Similar to YOLO but use multiple feature maps (grid sizes). Faster, Stronger
than YOLO v1.

\subsection{Image segmentation}

\includegraphics[width=0.5\textwidth]{Figures/segmentation-example.png}

In one sense this is a simple extension of classification: but one prediction
per pixel rather than one prediction per picture.

With segmenation, there is a tension between `semantics and location.'
\textit{Semantics} is the question of what, \textit{location} asks where.
Local information helps answer the former and global information helps answer
the latter. This need for two types of information gave rise to complicated
models prior to the era of fully end to end differentiable models. This
tension also explains the innovations beyond basic CNNs used for
classification.

Old methods: classifier applied to each object location and scale.
See~\cite{NIPS2015_5852}.

\paragraph{First fully convnet (trained end to end) segmenter}

BTW, very good write up on convnets.~\cite{long2015fully}

\includegraphics[width=0.5\textwidth]{Figures/long-segmentation.png}
\begin{itemize}
    \item uses in network upsampling
    \item doesn't require complicated chaining of pieces
    \item can use pre trained models
    \item use a skip architecture to combine coarse semantic information and shallow
appearance information.
\end{itemize}
There is a super easy way to make a simple CNN classifier a segmenter: make
the fully connected layers have a different height and width for each of the
4096 components.

An issue: by the time you've gotten to predicting a pixel, you've lost a lot
of the information on the surrounding pixels. The authors get around this by
bringing intermediate layers back into the prediction.

\includegraphics[width=0.75\textwidth]{Figures/skip-segmentation.png}

Some more~\cite{chen2018, NIPS2015_5852}

\paragraph{DeepLab}~\cite{chen2018}

\begin{itemize}
    \item convolution with upsampled filters --- allows control of resolution
        (or the context without increasing the number of parameters)
    \item spatial pyramid pooling: segment at multiple scales
    \item use conditional random fields to improve location accuracy (max
        pooling and down sampling lower locational accuracy)
\end{itemize}

They do this upsampling \textit{algorithme a trous} thing that~\cite{NIPS2015_5852}
do (i think).

\paragraph{Learning to segment object candidates}~\cite{NIPS2015_5852}

A model trained with two objectives
A convnet that branches out into two objecives.

\begin{enumerate}
    \item given an image patch ---  output a class agnostic segmentation mask
    \item the likelihood of the patch being centered on a full object
\end{enumerate}

Model generalizes to unseen categories it has not seen during training. Strong
work.

\subsection{Analysis with satellite images}

Mexico~\cite{babenko2017poverty} (There are two WB co-authors)

Poverty mapping~\cite{Jean790} Xie

Population~\cite{doupe2016, robinson2017}

Private sector~\cite{facebook, cnn_orbital}

\subsection{Spacenet}

https://github.com/SpaceNetChallenge

Issue: collecting multiple buildings in small areas
https://i.ho.lc/winning-solution-for-the-spacenet-challenge-joint-learning-with-openstreetmap.html

\section{Satellite data}

Planet Labs: \url{https://www.planet.com/}
Planet Labs free datasets:
~\url{https://www.planet.com/disasterdata/datasets/}
    ``We provide limited access to Explorer for up to 30 days to qualified disaster
    volunteer organizations, humanitarian organizations, and other coordinating
    bodies.''
3-5 meter imagery anywhere in the world. This may be too large for small
buildings.

**Insurance companies do this** ~~ Planet labs e book.
Predicting claim amounts // categorize unaffected assets, damaged or requiring
assessor // 
Nice image of water damage. I believe there is a way of measuring this from
space.

Satellogic has images with 30 bands. And are open for humanitarian efforts. ~\url{https://www.satellogic.com/commitment-to-science}

TellusLabs do crop monitoring and forecasting. VanderSat also on land use

Spaceknow can spot cars, buildings, etc
~\url{https://www.spaceknow.com/satellite-ai/}

Quasi free: Bing

Urban environments https://github.com/adrianalbert/urban-environments/tree/master/dataset-collection

\subsection{Expensive sources}

Digital Globe, Planet Labs, Own drone data

\subsection{Labelled data}

Spacenet: https://spacenetchallenge.github.io/datasets/datasetHomePage.html

\subsection{Augmentation}

Talk about data augmentation. For example, U-Net used this.

\section{Recommendations}

\appendix

\section{VGG net}
https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3

\begin{minted}{python}
from keras.models import Sequential
from keras.layers.core import Flatten, Dense, Dropout
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD
import cv2, numpy as np

def VGG_16(weights_path=None):
    model = Sequential()
    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(Flatten())
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1000, activation='softmax'))

    if weights_path:
        model.load_weights(weights_path)

    return model

if __name__ == "__main__":
    im = cv2.resize(cv2.imread('cat.jpg'), (224, 224)).astype(np.float32)
    im[:,:,0] -= 103.939
    im[:,:,1] -= 116.779
    im[:,:,2] -= 123.68
    im = im.transpose((2,0,1))
    im = np.expand_dims(im, axis=0)

    # Test pretrained model
    model = VGG_16('vgg16_weights.h5')
    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(optimizer=sgd, loss='categorical_crossentropy')
    out = model.predict(im)
    print np.argmax(out)
\end{minted}
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\renewcommand{\refname}{\spacedlowsmallcaps{References}} 

\bibliographystyle{unsrt}

\bibliography{review.bib}

%----------------------------------------------------------------------------------------

\end{document}
